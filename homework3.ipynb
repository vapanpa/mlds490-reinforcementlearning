{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1 : Question 1",
   "id": "1a8fd6eb91a13a14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "NUM_CLASSES = 62          # Labels are 0–61\n",
    "DATA_DIR = \"HW3-data\"     # folder containing the npy files\n",
    "\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train_data.npy\")\n",
    "TEST_PATH  = os.path.join(DATA_DIR, \"test_data.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load data\n",
    "# -------------------------------\n",
    "def load_data(train_path=TRAIN_PATH, test_path=TEST_PATH):\n",
    "    \"\"\"\n",
    "    Load federated train and global test data.\n",
    "\n",
    "    train_data[i]['images'], train_data[i]['labels'] give the local data\n",
    "    for client i, as described in the README.\n",
    "    \"\"\"\n",
    "    train_data = np.load(train_path, allow_pickle=True)\n",
    "    test_data = np.load(test_path, allow_pickle=True)\n",
    "    print(f\"Loaded train_data with {len(train_data)} clients.\")\n",
    "    print(f\"Loaded test_data with shape: {test_data[0]['images'].__class__}, \"\n",
    "          f\"{len(test_data[0]['images'])} test samples.\")\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Distribution helpers\n",
    "# -------------------------------\n",
    "def get_global_distribution(train_data, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Compute the class distribution aggregated across ALL clients.\n",
    "    Returns (counts, probabilities).\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "    for client in train_data:\n",
    "        all_labels.extend(client[\"labels\"])\n",
    "    all_labels = np.array(all_labels, dtype=int)\n",
    "\n",
    "    counts = np.bincount(all_labels, minlength=num_classes)\n",
    "    probs = counts / counts.sum()\n",
    "    return counts, probs\n",
    "\n",
    "\n",
    "def get_client_distribution(train_data, client_id, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Compute the class distribution for a single client.\n",
    "    Returns (counts, probabilities).\n",
    "    \"\"\"\n",
    "    labels = np.array(train_data[client_id][\"labels\"], dtype=int)\n",
    "    counts = np.bincount(labels, minlength=num_classes)\n",
    "    probs = counts / counts.sum()\n",
    "    return counts, probs\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Plotting functions\n",
    "# -------------------------------\n",
    "def plot_global_distribution(global_counts):\n",
    "    \"\"\"\n",
    "    Bar plot of global class counts.\n",
    "    \"\"\"\n",
    "    classes = np.arange(len(global_counts))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(classes, global_counts)\n",
    "    plt.xlabel(\"Class label\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Global class distribution across all clients\")\n",
    "    plt.xticks(classes)  # you can thin these out if it's too crowded\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_client_distributions(train_data, client_ids, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Bar plots of per-client class distributions for the given list of client_ids.\n",
    "    \"\"\"\n",
    "    num_clients = len(client_ids)\n",
    "    classes = np.arange(num_classes)\n",
    "\n",
    "    plt.figure(figsize=(12, 3 * num_clients))\n",
    "\n",
    "    for idx, cid in enumerate(client_ids, start=1):\n",
    "        labels = np.array(train_data[cid][\"labels\"], dtype=int)\n",
    "        counts = np.bincount(labels, minlength=num_classes)\n",
    "\n",
    "        ax = plt.subplot(num_clients, 1, idx)\n",
    "        ax.bar(classes, counts)\n",
    "        ax.set_title(f\"Client {cid} class distribution (n={labels.size})\")\n",
    "        ax.set_xlabel(\"Class label\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Main: runs Part 1, Q1 analysis\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load data\n",
    "    train_data, test_data = load_data()\n",
    "\n",
    "    num_clients = len(train_data)\n",
    "    print(f\"\\nNumber of clients in training set: {num_clients}\")\n",
    "\n",
    "    # 2) Global distribution across all clients\n",
    "    global_counts, global_probs = get_global_distribution(train_data, NUM_CLASSES)\n",
    "\n",
    "    print(\"\\n=== Global Class Distribution ===\")\n",
    "    print(\"Total number of training samples:\", global_counts.sum())\n",
    "    print(\"Counts per class (0–61):\")\n",
    "    print(global_counts)\n",
    "    print(\"\\nProportions per class (0–61):\")\n",
    "    print(np.round(global_probs, 4))\n",
    "\n",
    "    # 3) Example individual users (you can pick any 5 IDs you want)\n",
    "    example_clients = [0, 1, 2, 3, 4]  # change these if you prefer different clients\n",
    "\n",
    "    for cid in example_clients:\n",
    "        counts, probs = get_client_distribution(train_data, cid, NUM_CLASSES)\n",
    "        print(f\"\\n=== Client {cid} ===\")\n",
    "        print(f\"Number of local samples: {counts.sum()}\")\n",
    "\n",
    "        # Show the top-5 most common classes for this client\n",
    "        top5_idx = counts.argsort()[::-1][:5]\n",
    "        print(\"Top 5 classes by count:\")\n",
    "        for k in top5_idx:\n",
    "            if counts[k] > 0:\n",
    "                print(f\"  Class {k}: {counts[k]} samples ({probs[k]:.3%})\")\n",
    "\n",
    "    # 4) Plots for your report\n",
    "    plot_global_distribution(global_counts)\n",
    "    plot_client_distributions(train_data, example_clients, NUM_CLASSES)\n"
   ],
   "id": "f3f938e11fde3694"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1 : Question 2",
   "id": "854d28ad1c7a2772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import ray\n",
    "\n",
    "# =============================\n",
    "# Config\n",
    "# =============================\n",
    "NUM_CLASSES = 62\n",
    "DATA_DIR = \"HW3-data\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train_data.npy\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test_data.npy\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LOCAL_EPOCHS = 2          # you can tune this\n",
    "LR = 0.01\n",
    "NUM_ROUNDS = 50           # number of FedAvg communication rounds\n",
    "CLIENTS_PER_ROUND = 4     # required for Ray part\n",
    "\n",
    "VAL_SPLIT = 0.2           # 80/20 train/val per client\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# =============================\n",
    "# Seeding for reproducibility\n",
    "# =============================\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Data loading helpers\n",
    "# =============================\n",
    "def load_federated_data(train_path=TRAIN_PATH, test_path=TEST_PATH):\n",
    "    train_data = np.load(train_path, allow_pickle=True)\n",
    "    test_data = np.load(test_path, allow_pickle=True)\n",
    "\n",
    "    print(f\"Loaded train_data with {len(train_data)} clients.\")\n",
    "    print(f\"Loaded test_data with {len(test_data[0]['images'])} test samples.\")\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def split_client_data(images, labels, val_split=VAL_SPLIT):\n",
    "    \"\"\"\n",
    "    Split a single client's data into train/val sets (80/20 by default).\n",
    "    \"\"\"\n",
    "    num_samples = len(images)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_point = int(num_samples * (1.0 - val_split))\n",
    "    train_idx = indices[:split_point]\n",
    "    val_idx = indices[split_point:]\n",
    "\n",
    "    train_images = images[train_idx]\n",
    "    train_labels = labels[train_idx]\n",
    "    val_images = images[val_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "\n",
    "    return train_images, train_labels, val_images, val_labels\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images.astype(np.float32) / 255.0  # normalize to [0,1]\n",
    "        self.labels = labels.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.images[idx]  # shape (28, 28)\n",
    "        x = torch.from_numpy(x).view(-1)  # flatten to 784\n",
    "        y = torch.tensor(self.labels[idx])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Model definition\n",
    "# =============================\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    2-layer fully connected network:\n",
    "    784 -> 128 -> 62 with ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=28*28, hidden_dim=128, num_classes=NUM_CLASSES):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Evaluation helper\n",
    "# =============================\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            total_loss += loss.item() * y.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Ray Actor for a single client\n",
    "# =============================\n",
    "@ray.remote(num_cpus=1, num_gpus=0)  # set num_gpus=1 if you want one GPU per actor\n",
    "class ClientTrainer:\n",
    "    def __init__(self, train_images, train_labels, val_images, val_labels,\n",
    "                 batch_size=BATCH_SIZE, local_epochs=LOCAL_EPOCHS, lr=LR):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.train_dataset = ImageDataset(train_images, train_labels)\n",
    "        self.val_dataset = ImageDataset(val_images, val_labels)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.local_epochs = local_epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        # model will be recreated each round from global weights\n",
    "        self.input_dim = 28 * 28\n",
    "        self.hidden_dim = 128\n",
    "        self.num_classes = NUM_CLASSES\n",
    "\n",
    "        self.num_train_samples = len(self.train_dataset)\n",
    "\n",
    "    def train_one_round(self, global_state_dict):\n",
    "        \"\"\"\n",
    "        Receive global weights, train locally for a few epochs,\n",
    "        and return updated weights + train/val metrics.\n",
    "        \"\"\"\n",
    "        model = SimpleNet(self.input_dim, self.hidden_dim, self.num_classes).to(self.device)\n",
    "        model.load_state_dict(global_state_dict)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=self.lr)\n",
    "\n",
    "        # ---- Local training ----\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for epoch in range(self.local_epochs):\n",
    "            for x, y in self.train_loader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item() * y.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                total_train_correct += (preds == y).sum().item()\n",
    "                total_train_samples += y.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / total_train_samples\n",
    "        train_acc = total_train_correct / total_train_samples\n",
    "\n",
    "        # ---- Validation ----\n",
    "        val_loss, val_acc = evaluate_model(model, self.val_loader, self.device)\n",
    "\n",
    "        # Move weights back to CPU for Ray to transport\n",
    "        updated_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "        metrics = {\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"num_train_samples\": total_train_samples,\n",
    "        }\n",
    "\n",
    "        return updated_state_dict, metrics\n",
    "\n",
    "\n",
    "# =============================\n",
    "# FedAvg aggregation\n",
    "# =============================\n",
    "def fedavg_aggregate(client_states, client_sizes):\n",
    "    \"\"\"\n",
    "    Weighted average of client model parameters.\n",
    "    client_states: list of state_dicts (one per client)\n",
    "    client_sizes:  list of number of training samples (same order)\n",
    "    \"\"\"\n",
    "    total_samples = sum(client_sizes)\n",
    "    global_state = {}\n",
    "\n",
    "    # initialize with zeros\n",
    "    for key in client_states[0].keys():\n",
    "        global_state[key] = torch.zeros_like(client_states[0][key])\n",
    "\n",
    "    for state, n_k in zip(client_states, client_sizes):\n",
    "        weight = n_k / total_samples\n",
    "        for key in global_state.keys():\n",
    "            global_state[key] += state[key] * weight\n",
    "\n",
    "    return global_state\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Main FedAvg with Ray\n",
    "# =============================\n",
    "def main():\n",
    "    # --- Init Ray ---\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "    # --- Load federated data ---\n",
    "    train_data, test_data = load_federated_data()\n",
    "    num_clients = len(train_data)\n",
    "    print(f\"Number of clients: {num_clients}\")\n",
    "\n",
    "    # --- Pre-split per-client data into train/val ---\n",
    "    client_splits = []\n",
    "    for client_id in range(num_clients):\n",
    "        images = train_data[client_id][\"images\"]\n",
    "        labels = train_data[client_id][\"labels\"]\n",
    "        tr_img, tr_lbl, val_img, val_lbl = split_client_data(images, labels, VAL_SPLIT)\n",
    "        client_splits.append((tr_img, tr_lbl, val_img, val_lbl))\n",
    "\n",
    "    # --- Prepare global test loader ---\n",
    "    test_images = test_data[0][\"images\"].astype(np.float32) / 255.0\n",
    "    test_labels = test_data[0][\"labels\"].astype(np.int64)\n",
    "\n",
    "    test_dataset = ImageDataset(test_images, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # --- Initialize global model ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    global_model = SimpleNet().to(device)\n",
    "    global_state = {k: v.cpu() for k, v in global_model.state_dict().items()}\n",
    "\n",
    "    # --- Tracking metrics across rounds ---\n",
    "    round_train_losses = []\n",
    "    round_val_losses = []\n",
    "    round_train_accs = []\n",
    "    round_val_accs = []\n",
    "\n",
    "    for rnd in range(1, NUM_ROUNDS + 1):\n",
    "        print(f\"\\n=== Communication Round {rnd}/{NUM_ROUNDS} ===\")\n",
    "\n",
    "        # Sample clients for this round\n",
    "        selected_clients = random.sample(range(num_clients), CLIENTS_PER_ROUND)\n",
    "        print(f\"Selected clients: {selected_clients}\")\n",
    "\n",
    "        # Create Ray actors for selected clients\n",
    "        actors = []\n",
    "        for cid in selected_clients:\n",
    "            tr_img, tr_lbl, val_img, val_lbl = client_splits[cid]\n",
    "            actor = ClientTrainer.remote(tr_img, tr_lbl, val_img, val_lbl)\n",
    "            actors.append(actor)\n",
    "\n",
    "        # Broadcast current global model weights to clients and start training\n",
    "        global_state_copy = {k: v.clone() for k, v in global_state.items()}  # safety copy\n",
    "\n",
    "        futures = [\n",
    "            actor.train_one_round.remote(global_state_copy)\n",
    "            for actor in actors\n",
    "        ]\n",
    "\n",
    "        results = ray.get(futures)\n",
    "\n",
    "        client_states = []\n",
    "        client_sizes = []\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "\n",
    "        for (updated_state, metrics), cid in zip(results, selected_clients):\n",
    "            client_states.append(updated_state)\n",
    "            client_sizes.append(metrics[\"num_train_samples\"])\n",
    "            train_losses.append(metrics[\"train_loss\"])\n",
    "            train_accs.append(metrics[\"train_acc\"])\n",
    "            val_losses.append(metrics[\"val_loss\"])\n",
    "            val_accs.append(metrics[\"val_acc\"])\n",
    "\n",
    "            print(\n",
    "                f\"Client {cid}: \"\n",
    "                f\"train_loss={metrics['train_loss']:.4f}, \"\n",
    "                f\"train_acc={metrics['train_acc']:.4f}, \"\n",
    "                f\"val_loss={metrics['val_loss']:.4f}, \"\n",
    "                f\"val_acc={metrics['val_acc']:.4f}\"\n",
    "            )\n",
    "\n",
    "        # FedAvg aggregation step\n",
    "        global_state = fedavg_aggregate(client_states, client_sizes)\n",
    "\n",
    "        # Log aggregated (simple mean of metrics across selected clients)\n",
    "        avg_train_loss = float(np.mean(train_losses))\n",
    "        avg_val_loss = float(np.mean(val_losses))\n",
    "        avg_train_acc = float(np.mean(train_accs))\n",
    "        avg_val_acc = float(np.mean(val_accs))\n",
    "\n",
    "        round_train_losses.append(avg_train_loss)\n",
    "        round_val_losses.append(avg_val_loss)\n",
    "        round_train_accs.append(avg_train_acc)\n",
    "        round_val_accs.append(avg_val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"[Round {rnd}] Avg Train Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Avg Val Loss: {avg_val_loss:.4f}, \"\n",
    "            f\"Avg Train Acc: {avg_train_acc:.4f}, \"\n",
    "            f\"Avg Val Acc: {avg_val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "    # --- Load final global weights into model and evaluate on test set ---\n",
    "    global_model.load_state_dict(global_state)\n",
    "    global_model.to(device)\n",
    "\n",
    "    test_loss, test_acc = evaluate_model(global_model, test_loader, device)\n",
    "    print(f\"\\n=== Final Test Performance ===\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # --- Plot metrics across rounds ---\n",
    "    rounds = np.arange(1, NUM_ROUNDS + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(rounds, round_train_losses, label=\"Train Loss\")\n",
    "    plt.plot(rounds, round_val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Communication Round\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Train/Validation Loss vs Rounds (Ray FedAvg)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(rounds, round_train_accs, label=\"Train Accuracy\")\n",
    "    plt.plot(rounds, round_val_accs, label=\"Val Accuracy\")\n",
    "    plt.xlabel(\"Communication Round\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Train/Validation Accuracy vs Rounds (Ray FedAvg)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Shutdown Ray\n",
    "    ray.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "de02112b0359f92b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
