{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Name: Victor Olawale-Apanpa\n",
    "\n",
    "DATE: 19 NOV 2025\n",
    "\n",
    " - Github Username: vapanpa\n",
    "  - How to Run: Notebook should be in the same directory as 'HW3-data' folder.\n",
    "\n",
    " Run as is to produce output folders 'output/part1' and 'output/part2' with all required plots."
   ],
   "id": "ba91c6f44000354c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Federated Learning and Differential Privacy — Report\n",
    "\n",
    "Project: FedAvg + Differential Privacy Analysis\n",
    "\n",
    "⸻\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Federated Learning (FL) allows multiple clients to collaboratively train a global model without sharing raw data. Instead, each client trains locally and sends model updates to a central server. This preserves privacy and reduces communication of sensitive information.\n",
    "\n",
    "In this project, we implement:\n",
    "\t1.\tFedAvg (Non-DP) — baseline federated averaging\n",
    "\t2.\tDifferentially Private FedAvg — adding Laplace noise to client model updates\n",
    "\n",
    "We evaluate:\n",
    "\t•\tModel convergence behavior\n",
    "\t•\tPrivacy–utility tradeoffs\n",
    "\t•\tImpact of noise levels on training stability and performance\n",
    "\n",
    "⸻\n",
    "\n",
    "## 2. Dataset and Non-IID Client Distribution\n",
    "\n",
    "The training dataset is partitioned across 100 clients in a non-IID (label-skewed) manner.\n",
    "Below are sample visualizations of the label distributions for clients 0–4.\n",
    "\n",
    "Client Label Distributions\n",
    "\n",
    "![Alt text](output/part1/part1_client_0_label_hist.png \"Client 0 Label Distribution\")\n",
    "![Alt text](output/part1/part1_client_1_label_hist.png \"Client 1 Label Distribution\")\n",
    "![Alt text](output/part1/part1_client_2_label_hist.png \"Client 2 Label Distribution\")\n",
    "![Alt text](output/part1/part1_client_3_label_hist.png \"Client 3 Label Distribution\")\n",
    "![Alt text](output/part1/part1_client_4_label_hist.png \"Client 4 Label Distribution\")\n",
    "\n",
    "Global Label Distribution\n",
    "\n",
    "![Alt text](output/part1/part1_global_label_hist.png \"Global Label Distribution\")\n",
    "\n",
    "Observation:\n",
    "\t•\tEach client has a unique, uneven label distribution.\n",
    "\t•\tSome labels are heavily overrepresented for certain clients.\n",
    "\t•\tThis confirms a highly non-IID scenario, which makes FedAvg training more challenging."
   ],
   "id": "e718f0810f7ae0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Part 1 — FedAvg (No Differential Privacy)\n",
    "\n",
    "### 3.1 Model Convergence\n",
    "\n",
    "Below are the accuracy and loss curves for standard FedAvg.\n",
    "\n",
    "FedAvg Accuracy vs Rounds\n",
    "![Alt text](output/part1/part1_cf0.100_e1_acc.png \"FedAvg Accuracy vs Rounds\")\n",
    "\n",
    "FedAvg Loss vs Rounds\n",
    "![Alt text](output/part1/part1_cf0.100_e1_loss.png \"FedAvg Loss vs Rounds\")\n",
    "\n",
    "Analysis\n",
    "\t•\tTraining accuracy and test accuracy increase steadily across rounds.\n",
    "\t•\tFinal test accuracy reaches ~0.63.\n",
    "\t•\tLoss decreases smoothly, indicating stable convergence.\n",
    "\t•\tThis demonstrates the baseline performance without privacy noise.\n",
    "\n",
    "FedAvg successfully learns despite non-IID distributions."
   ],
   "id": "16cbc6f7a8347fdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Part 2 — Differential Privacy (DP-FedAvg)\n",
    "\n",
    "In this section, we add Laplace noise with scale parameter b to each client’s model update before aggregation.\n",
    "\n",
    "### 4.1 DP Training (Example: b = 0.1)\n",
    "\n",
    "DP Loss Curve (b = 0.1)\n",
    "![Alt text](output/part2/part2_b000_loss.png \"FedAvg Loss vs Rounds\")\n",
    "\n",
    "\n",
    "DP Accuracy Curve (b = 0.1)\n",
    "![Alt text](output/part2/part2_b000_acc.png \"FedAvg Accuracy vs Rounds\")\n",
    "\n",
    "Analysis\n",
    "\t•\tLoss decreases early on but eventually plateaus or increases slightly.\n",
    "\t•\tAccuracy is lower than FedAvg and improves more slowly.\n",
    "\t•\tNoise introduces instability, especially in early rounds.\n",
    "\n",
    "Still, the model learns meaningful structure even under DP."
   ],
   "id": "9cb05c96a3fb592f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Privacy–Utility Tradeoff Analysis\n",
    "\n",
    "We evaluate model utility (final test accuracy) at four different Laplace noise levels:\n",
    "\t•\tb = 0.0 (no privacy)\n",
    "\t•\tb = 0.01\n",
    "\t•\tb = 0.05\n",
    "\t•\tb = 0.1\n",
    "\n",
    "Final Accuracy vs Noise Scale b\n",
    "![Alt text](output/part2/part2_dp_final_acc_vs_b.png \"FedAvg Loss vs Rounds\")\n",
    "\n",
    "Interpretation\n",
    "\t•\tb = 0.0 (no noise) achieves the highest accuracy (~0.63).\n",
    "\t•\tAccuracy drops sharply at b = 0.01, indicating strong privacy → strong utility loss.\n",
    "\t•\tIncreasing noise from 0.01 → 0.1 slightly improves accuracy, showing the model can tolerate moderate noise once it stabilizes.\n",
    "\t•\tHowever, all DP levels underperform the baseline.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "Higher noise improves privacy but reduces accuracy.\n",
    "There is a clear privacy–utility tradeoff."
   ],
   "id": "b536701d7681f584"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Final Summary\n",
    "\n",
    "FedAvg (No DP)\n",
    "\t•\tAchieves stable convergence and highest accuracy.\n",
    "\t•\tHandles non-IID data reasonably well.\n",
    "\n",
    "Differentially Private FedAvg\n",
    "\t•\tAdding Laplace noise reduces accuracy.\n",
    "\t•\tLower noise (b=0.01) hurts performance the most.\n",
    "\t•\tModerate noise (b=0.05–0.1) allows limited recovery.\n",
    "\t•\tOverall: privacy comes at a meaningful cost to model utility.\n",
    "\n",
    "Key Takeaways\n",
    "\t•\tFL works well in label-skewed environments but benefits from privacy-aware strategies.\n",
    "\t•\tDifferential privacy is effective but requires careful tuning.\n",
    "\t•\tThe tradeoff between privacy and accuracy must be balanced depending on the application.\n"
   ],
   "id": "af3513036190fe37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
